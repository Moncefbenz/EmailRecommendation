# -*- coding: utf-8 -*-
"""topic_modelling_gensim_p2_5_15/11/18.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BHwOgbbWXTUH2t8tozE03meucMeLEK-e
"""

!curl --header "Host: storage.googleapis.com" --header "User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.67 Safari/537.36" --header "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8" --header "Accept-Language: en-GB,en-US;q=0.9,en;q=0.8" --header "Cookie: _ga=GA1.3.1223421806.1538226806" --header "Connection: keep-alive" "https://storage.googleapis.com/kaggle-datasets/14132/19029/gonuts.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1540566428&Signature=r0gaK%2FREDIzaH8DFUtK%2Fr%2B0zTK7h0ltFOUGfLJSgWBQpJF1ZfJZV5R5ZaB5HZ1K0nvU2GYvxMohSnnMdT50heiIdsHvkOWAitHFqe7nKgERixpOQIRXaEeUXFyQV73qG5L4aSTEot0Pr5MsmXpbtLiqwNEQVGXzqSI8hXCsuPtW6VNO85%2Fq4z1RJTxnBg1me4THm9JSyiAWNih2H%2FvbqiovUm2xjrrsvl7mpjP6Q2v%2FX1XD2A3mmYyUvjPyY8IsvjTKiReWvz%2BGJJE66tgAH34mldG3E9BH0kk02VYImkn%2F1UyJdtFnikwcSaPXfAHkF94q8KP3KzFVMXJLagZk63w%3D%3D" -o "gonuts.zip" -L

!curl --header "Host: storage.googleapis.com" --header "User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36" --header "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8" --header "Accept-Language: en-GB,en-US;q=0.9,en;q=0.8" --header "Cookie: _ga=GA1.3.1223421806.1538226806" --header "Connection: keep-alive" "https://storage.googleapis.com/kaggle-datasets/14132/19029/golang-nuts.tar.xz.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1542519893&Signature=Gr7DlKBPQN19Tv5d%2FnHhi%2FKXkj8%2BfurtnlvDB1xet936rbeZ5sOfbL0%2Fl20kjO4q7lX2MMKIEuowlx2%2BuFT5rqaSAlW7FAUJGSx6Jb0z43LieuA0zoQIlw97PTPQBoU5EOXVqI2EvMG6CAhGNOoEn8CbDv0%2F8JpAYFV7J%2FNMeGCA8Mzu35JHSuHNAIFeszyX5zTY1%2FDKLSdYchQpFbgTuPhP9Ilsq1ThfwnsqcJyOoJJx9gV5nbFV7epwaRIH1izhM9m4Zhg1dF8qXQ%2B8PPgGDIqBtM18hoPm6hC73cqdH2prLdl3WDBzv40RliPFU%2F1V1Yh8hlK23cDU6NvyBE%2BvQ%3D%3D" -o "golang-nuts.tar.xz.zip" -L

!wget --header="Host: storage.googleapis.com" --header="User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8" --header="Accept-Language: en-IN,en;q=0.9,de-DE;q=0.8,de;q=0.7,en-GB;q=0.6,en-US;q=0.5,mr;q=0.4" "https://storage.googleapis.com/kaggle-datasets/14132/19029/golang-nuts.tar.xz.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1541473722&Signature=qa%2FK%2F4TBXebtWOApaV4jI6HUTUqUDahwLOdJsc6ScSeC349Km5GxvICYS6g1ZmAdLBWTSSxGKVhvPmvmsPfkwoxy9X1JO3k8c9eiqgs%2BZs68zccWg2LgdiERMEmv%2Fab6T1B2%2FBsk9p2a2Cu%2B%2Bgz%2F1%2BPqgluH1FasHISVmkELPO48ZyTPo%2FGNJYJCzT0gE2GX%2BpDb32aV2uguJl08V94JvF8MbW2bmr%2B%2BiI7ExwLDyCdPqF7mQACOYLrL5quIoyXMBtAV570NlkDkW3fogfKTptNWNbUo6ztSRU8K7rfzEKsYXTaK9DJSq6xnhE8ZwPTSAgQ2NXG6dVAOcjyRqeFVwg%3D%3D" -O "golang-nuts.tar.xz.zip" -c

!rm go*

!ls

!apt-get install xz-utils

!unzip golang-nuts.tar.xz.zip

!tar -xf golang-nuts.tar.xz

!ls

!ls golang-nuts/

!mkdir q1 q2 q3 q4

!cp golang-nuts/j*/*.email q1

!ls q1/ | wc -l

!pip install flanker

import email, glob
import pandas as pd
from flanker import mime
import string


def remove_code(msg) :
	msg1 = ''
	cnt = 0
	list = ['func', 'import','break' ,'default','interface','select','case', 'defer','go','map','struct','chan','else', 'goto','package','switch','const','fallthrough','if','range','type','continue','for','import','return','var']
	for char in msg :
		if char == '{' :
			cnt += 1
		if char == '}' :
			cnt -= 1
		elif cnt == 0 :
			msg1 += char
		else :
			continue
	msg = msg1
	msg = msg.splitlines()
	msg1 = ''
	for line in msg :
		if line.strip() == '' :
# 			print("*******")
			continue
		flag = True
		for x in list :
			if x in line :
				flag = False
		if flag :
			msg1 += line + '\n'
	return (msg1)
#   return (msg1)
  
rt =''
fpath = "q1/*.email"
files = glob.glob(fpath)
for file in files :
  f = open(file, "r")
  msg = f.read()
#   print('###############')
#   print(msg)
#   print('###############')
  msg = remove_code(msg)
#   print('$$$$$$$$$$$$$$$')
#   print(msg)
#   print('$$$$$$$$$$$$$$$')
  msg = mime.from_string(msg)
#     print("printing email message!!")
  if msg.content_type.is_singlepart():
      temp = str(msg.body)
      temp = temp.splitlines()
      for _ in temp:
          if _.startswith('>'):
              continue
          elif _.startswith('On'):
              continue
          else:
              rt+=_+"\n"
#         print("************")
  else :
      for part in msg.parts :
          if "(text/plain)" in str(part) :
              temp = str(part.body)
              temp = temp.splitlines()
              for _ in temp :
                  if _.startswith('>') :
                      continue
                  if _.startswith('On'):
                      continue
                  else :
                      rt+=_+"\n"
#                 print("************")
                
# print(tt)

def remove_code(msg) :
	msg1 = ''
	cnt = 0
	list = ['func', 'import']
	for char in msg :
		if char == '{' :
			cnt += 1
		if char == '}' :
			cnt -= 1
		elif cnt == 0 :
			msg1 += char
		else :
			continue
	msg = msg1
	msg = msg.splitlines()
	msg1 = ''
	for line in msg :
		if line.strip() == '' :
			print("***")
			continue
		flag = True
		for x in list :
			if x in line :
				flag = False
		if flag :
			msg1 += line + '\n'
	return (msg1)

rt

tt = rt

import re
tt = [re.sub(r'^https?:\/\/.*[\r\n]*', '', ww, flags=re.MULTILINE) for ww in tt]
tt = [re.sub(r'^http?:\/\/.*[\r\n]*', '', ww, flags=re.MULTILINE) for ww in tt]
tt = [re.sub(r'\S*@\S*.com\s?', '', ww, flags=re.MULTILINE) for ww in tt]

tt = ''.join(tt)
tt

!pip install spacy

!pip install matplotlib
!pip install gensim
!pip install pyldavis

!python -m spacy download en

import matplotlib.pyplot as plt
import gensim
import numpy as np
import spacy

from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel
from gensim.models.wrappers import LdaMallet
from gensim.corpora import Dictionary
import pyLDAvis.gensim
from gensim import models

import os, re, operator, warnings
warnings.filterwarnings('ignore')  # Let's not pay heed to them right now
# %matplotlib inline

import spacy
nlp = spacy.load("en")

my_stop_words = ['gmail','google','github','yahoo','com','org','http','https','golang','www','regards','thanks','html5','tidyhtml5']
for stopword in my_stop_words:
    lexeme = nlp.vocab[stopword]
    lexeme.is_stop = True

ttp1 = tt

doc = nlp(ttp1)
print(doc)

doc

# we add some words to the stop word list
texts, article, skl_texts = [], [], []
for w in doc:
    # if it's not a stop word or punctuation mark, add it to our article!
    if w.text != '\n' and not w.is_stop and not w.is_punct and not w.like_num:
        # we add the lematized version of the word
        article.append(w.lemma_)
    # if it's a new line, it means we're onto our next document
    if w.text == '\n':
        print('----hi i am here----\n')
        skl_texts.append(' '.join(article))
        texts.append(article)
        article = []

print(texts)

bigram = gensim.models.Phrases(texts)

texts = [bigram[line] for line in texts]

texts[1][0:10]

dictionary = Dictionary(texts)
dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)

corpus = [dictionary.doc2bow(text) for text in texts]
tfidf = models.TfidfModel(corpus)
corpus = tfidf[corpus]

corpus[1][0:10]

lsimodel = LsiModel(corpus=corpus, num_topics=10, id2word=dictionary)

lsimodel.show_topics(num_topics=5)  # Showing only the top 5 topics

hdpmodel = HdpModel(corpus=corpus, id2word=dictionary)

hdpmodel.show_topics()

ldamodel = LdaModel(corpus=corpus, num_topics=10, id2word=dictionary)

ldamodel.show_topics()

import sklearn
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.datasets import fetch_20newsgroups
from sklearn.decomposition import NMF, LatentDirichletAllocation

def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print ("Topic :", (topic_idx))
        print (" ".join([feature_names[i]
                        for i in topic.argsort()[:-no_top_words - 1:-1]]))

# dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))
# documents = dataset.data

no_features = 1000

# NMF is able to use tf-idf
tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')
tfidf = tfidf_vectorizer.fit_transform(skl_texts)
tfidf_feature_names = tfidf_vectorizer.get_feature_names()

# LDA can only use raw term counts for LDA because it is a probabilistic graphical model
tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')
tf = tf_vectorizer.fit_transform(skl_texts)
tf_feature_names = tf_vectorizer.get_feature_names()

no_topics = 10

# Run NMF
nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)

# Run LDA
lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)

no_top_words = 10
display_topics(nmf, tfidf_feature_names, no_top_words)
display_topics(lda, tf_feature_names, no_top_words)

tfidf_feature_names[2], tf_feature_names[2]

for line in tf:
    print (line)
    break

pyLDAvis.enable_notebook()
pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)

def evaluate_graph(dictionary, corpus, texts, limit):
    """
    Function to display num_topics - LDA graph using c_v coherence
    
    Parameters:
    ----------
    dictionary : Gensim dictionary
    corpus : Gensim corpus
    limit : topic limit
    
    Returns:
    -------
    lm_list : List of LDA topic models
    c_v : Coherence values corresponding to the LDA model with respective number of topics
    """
    c_v = []
    lm_list = []
    for num_topics in range(1, limit):
        lm = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)
        lm_list.append(lm)
        cm = CoherenceModel(model=lm, texts=texts, dictionary=dictionary, coherence='c_v')
        c_v.append(cm.get_coherence())
        
    # Show graph
    x = range(1, limit)
    plt.plot(x, c_v)
    plt.xlabel("num_topics")
    plt.ylabel("Coherence score")
    plt.legend(("c_v"), loc='best')
    plt.show()
    
    return lm_list, c_v

"""%%time
lmlist, c_v = evaluate_graph(dictionary=dictionary, corpus=corpus, texts=texts, limit=10)
"""

def ret_top_model():
    """
    Since LDAmodel is a probabilistic model, it comes up different topics each time we run it. To control the
    quality of the topic model we produce, we can see what the interpretability of the best topic is and keep
    evaluating the topic model until this threshold is crossed. 
    
    Returns:
    -------
    lm: Final evaluated topic model
    top_topics: ranked topics in decreasing order. List of tuples
    """
    top_topics = [(0, 0)]
    while top_topics[0][1] < 0.97:
        lm = LdaModel(corpus=corpus, id2word=dictionary)
        coherence_values = {}
        for n, topic in lm.show_topics(num_topics=-1, formatted=False):
            topic = [word for word, _ in topic]
            cm = CoherenceModel(topics=[topic], texts=texts, dictionary=dictionary, window_size=10)
            coherence_values[n] = cm.get_coherence()
        top_topics = sorted(coherence_values.items(), key=operator.itemgetter(1), reverse=True)
    return lm, top_topics

lm, top_topics = ret_top_model()

print(top_topics[:5])

pprint([lm.show_topic(topicid) for topicid, c_v in top_topics[:10]])

lsitopics = [[word for word, prob in topic] for topicid, topic in lsimodel.show_topics(formatted=False)]

hdptopics = [[word for word, prob in topic] for topicid, topic in hdpmodel.show_topics(formatted=False)]

ldatopics = [[word for word, prob in topic] for topicid, topic in ldamodel.show_topics(formatted=False)]

lsi_coherence = CoherenceModel(topics=lsitopics[:10], texts=texts, dictionary=dictionary, window_size=10).get_coherence()

hdp_coherence = CoherenceModel(topics=hdptopics[:10], texts=texts, dictionary=dictionary, window_size=10).get_coherence()

lda_coherence = CoherenceModel(topics=ldatopics, texts=texts, dictionary=dictionary, window_size=10).get_coherence()

def evaluate_bar_graph(coherences, indices):
    """
    Function to plot bar graph.
    
    coherences: list of coherence values
    indices: Indices to be used to mark bars. Length of this and coherences should be equal.
    """
    assert len(coherences) == len(indices)
    n = len(coherences)
    x = np.arange(n)
    plt.bar(x, coherences, width=0.2, tick_label=indices, align='center')
    plt.xlabel('Models')
    plt.ylabel('Coherence Value')

evaluate_bar_graph([lsi_coherence, hdp_coherence, lda_coherence],
                   ['LSI', 'HDP', 'LDA'])


