{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email, glob\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from flanker import mime\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pprint import pprint\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from files Preprocessing\n",
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_content_in_braces(msg) :\n",
    "\tmsg1 = ''\n",
    "\tcnt = 0\n",
    "\tfor char in msg :\n",
    "\t\tif char == '{' :\n",
    "\t\t\tcnt += 1\n",
    "\t\telif char == '}' :\n",
    "\t\t\tcnt -= 1\n",
    "\t\telif cnt == 0 :\n",
    "\t\t\tmsg1 += char\n",
    "\t\telse :\n",
    "\t\t\tcontinue\n",
    "\treturn msg1\n",
    "\n",
    "def remove_func_and_struct(msg) :\n",
    "\tmsg1 = ''\n",
    "\ttake_line = True\n",
    "\tmsg = msg.splitlines()\n",
    "\tfor line in msg :\n",
    "\t\ttake_line = True\n",
    "\t\tif line == '' :\n",
    "\t\t\tcontinue\n",
    "\t\twords = line.split(' ')\n",
    "\t\tif words[0] == \"func\" :\n",
    "\t\t\ttake_line = False\n",
    "\t\telif words[0] == \"type\" :\n",
    "\t\t\tif len(words)  >= 3  and words[2] == \"struct\" :\n",
    "\t\t\t\ttake_line = False\n",
    "\t\tif take_line :\n",
    "\t\t\tmsg1 += (line + '\\n')\n",
    "\treturn msg1\n",
    "\n",
    "def remove_other_code_lines(msg) :\n",
    "\tmsg1 = ''\n",
    "\ttake_line = True\n",
    "\tmsg = msg.splitlines()\n",
    "\ti = 0\t\n",
    "\twhile i < len(msg) :\n",
    "\t\tif (msg[i] == '') or (\"//\" in msg[i]) :\n",
    "\t\t\ti += 1\n",
    "\t\t\tcontinue\n",
    "\t\ttake_line = True\n",
    "\t\tline = msg[i]\n",
    "\t\tif \"package\" in line :\n",
    "\t\t\twords = line.split(' ')\n",
    "\t\t\tif len(words) < 4 :\n",
    "\t\t\t\ttake_line = False\n",
    "\t\telif \"import\" in line :\n",
    "\t\t\twords = line.split(' ')\n",
    "\t\t\tif len(words) < 4 :\n",
    "\t\t\t\ttake_line = False\n",
    "\t\t\t\tif \"(\" in line :\n",
    "\t\t\t\t\twhile ')' not in msg[i] :\n",
    "\t\t\t\t\t\ti += 1\n",
    "\t\telif \"const\" in line :\n",
    "\t\t\twords = line.split(' ')\n",
    "\t\t\tif len(words) < 4 :\n",
    "\t\t\t\ttake_line = False\n",
    "\t\t\t\tif \"(\" in line :\n",
    "\t\t\t\t\twhile ')' not in msg[i] :\n",
    "\t\t\t\t\t\ti += 1\n",
    "\t\tif take_line :\n",
    "\t\t\tmsg1 += line + '\\n'\n",
    "\t\ti += 1\n",
    "\treturn msg1\n",
    "\n",
    "\n",
    "def remove_code(msg) :\n",
    "\tmsg = (remove_content_in_braces(msg))\n",
    "\tmsg = (remove_func_and_struct(msg))\n",
    "\tmsg = (remove_other_code_lines(msg))\n",
    "\treturn msg\n",
    "\n",
    "def get_header(msg):\n",
    "    msg = email.message_from_string(msg)\n",
    "    mfrom = msg['From'].split('<')[0]\n",
    "    return mfrom\n",
    "    \n",
    "def flan(msg):\n",
    "    rt = ''\n",
    "    msg = mime.from_string(msg)\n",
    "    if msg.content_type.is_singlepart():\n",
    "      temp = str(msg.body)\n",
    "      temp = temp.splitlines()\n",
    "      for _ in temp:\n",
    "          if _.startswith('>'):\n",
    "              continue\n",
    "          elif _.startswith('On'):\n",
    "              continue\n",
    "          else:\n",
    "              rt+=_+\"\\n\"\n",
    "    else :\n",
    "      for part in msg.parts :\n",
    "          if \"(text/plain)\" in str(part) :\n",
    "              temp = str(part.body)\n",
    "              temp = temp.splitlines()\n",
    "              for _ in temp :\n",
    "                  if _.startswith('>') :\n",
    "                      continue\n",
    "                  if _.startswith('On'):\n",
    "                      continue\n",
    "                  else :\n",
    "                      rt+=_+\"\\n\"\n",
    "    return rt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading files, tokenization & finding total_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt =''\n",
    "fpath = \"/home/niki/Documents/BE_Project/my_EmailRecommmendation/features/Dataset/*/*.email\"\n",
    "files = glob.glob(fpath)\n",
    "for file in files :\n",
    "  f = open(file, \"r\")\n",
    "  msg = f.read()\n",
    "  msg = mime.from_string(msg)\n",
    "  if msg.content_type.is_singlepart():\n",
    "      temp = str(msg.body)\n",
    "      temp = temp.splitlines()\n",
    "      for _ in temp:\n",
    "          if _.startswith('>'):\n",
    "              continue\n",
    "          elif _.startswith('On'):\n",
    "              continue\n",
    "          else:\n",
    "              rt+=_+\"\\n\"\n",
    "  else :\n",
    "      for part in msg.parts :\n",
    "          if \"(text/plain)\" in str(part) :\n",
    "              temp = str(part.body)\n",
    "              temp = temp.splitlines()\n",
    "              for _ in temp :\n",
    "                  if _.startswith('>') :\n",
    "                      continue\n",
    "                  if _.startswith('On'):\n",
    "                      continue\n",
    "                  else :\n",
    "                      rt+=_+\"\\n\"\n",
    "               \n",
    "rt = remove_code(rt)\n",
    "# print(rt)\n",
    "rt = rt.split('\\n')\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(rt)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "max_words=total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe, apply functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['body','replier', 'thread_no'])\n",
    "users = []\n",
    "fpath = \"/home/niki/Documents/BE_Project/my_EmailRecommmendation/features/Dataset/*\"\n",
    "folder = glob.glob(fpath)\n",
    "th_no = 0\n",
    "\n",
    "for fol in folder:\n",
    "    files = glob.glob(fol+'/*.email')\n",
    "    flag = 0\n",
    "    t = ''\n",
    "    for file in files:\n",
    "        if flag==0:\n",
    "            data = open(file,'r')\n",
    "            temp = data.read()\n",
    "            header = get_header(temp)\n",
    "            temp = flan(temp)\n",
    "            temp = remove_code(temp)\n",
    "            t = temp\n",
    "            flag = 1\n",
    "            users.append(header)\n",
    "            \n",
    "            continue\n",
    "        data = open(file,'r')\n",
    "        temp = data.read()\n",
    "        header = get_header(temp)\n",
    "        users.append(header)\n",
    "        temp = flan(temp)\n",
    "        temp = remove_code(temp)\n",
    "        df = df.append({'body': t,'replier':header, 'thread_no':th_no}, ignore_index=True)\n",
    "        t = t + temp\n",
    "    th_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                  body  \\\n",
      "0   I have client and server.\\n--- Client\\ntype\\tA...   \n",
      "1   I'm looking at the math package and I'm not se...   \n",
      "2   I'm looking at the math package and I'm not se...   \n",
      "3   once you get the whole request, you can then u...   \n",
      "4   once you get the whole request, you can then u...   \n",
      "5   i felt that this is necessary though i still f...   \n",
      "6   This is true at the moment, but something we i...   \n",
      "7   This is true at the moment, but something we i...   \n",
      "8   This is true at the moment, but something we i...   \n",
      "9   This is true at the moment, but something we i...   \n",
      "10  This is true at the moment, but something we i...   \n",
      "11  The following crude hack worked for me:\\nApply...   \n",
      "12  The following crude hack worked for me:\\nApply...   \n",
      "13  The following crude hack worked for me:\\nApply...   \n",
      "14  The following crude hack worked for me:\\nApply...   \n",
      "15  The following crude hack worked for me:\\nApply...   \n",
      "16  The following crude hack worked for me:\\nApply...   \n",
      "17  The following crude hack worked for me:\\nApply...   \n",
      "18  Google \"IMS T424\", \"IMS T800\", \"IMST414\" :-)\\n...   \n",
      "19  Google \"IMS T424\", \"IMS T800\", \"IMST414\" :-)\\n...   \n",
      "20  Google \"IMS T424\", \"IMS T800\", \"IMST414\" :-)\\n...   \n",
      "21  Google \"IMS T424\", \"IMS T800\", \"IMST414\" :-)\\n...   \n",
      "22  I have no problem doing the following toy code...   \n",
      "23  I have no problem doing the following toy code...   \n",
      "24  I have no problem doing the following toy code...   \n",
      "25  I have no problem doing the following toy code...   \n",
      "26  I have no problem doing the following toy code...   \n",
      "27  I have no problem doing the following toy code...   \n",
      "28  I have no problem doing the following toy code...   \n",
      "29  Be \"go language\" going to support  android ui ...   \n",
      "..                                                ...   \n",
      "38  I wrote a generator and divided the complex th...   \n",
      "39  go_spec.html#Order_of_evaluation)\\n\"When evalu...   \n",
      "40  go_spec.html#Order_of_evaluation)\\n\"When evalu...   \n",
      "41                                Got it, thanks!!!\\n   \n",
      "42  Got it, thanks!!!\\nOh sorry,  I pasted the old...   \n",
      "43  Got it, thanks!!!\\nOh sorry,  I pasted the old...   \n",
      "44  Got it, thanks!!!\\nOh sorry,  I pasted the old...   \n",
      "45  Got it, thanks!!!\\nOh sorry,  I pasted the old...   \n",
      "46                                             Russ\\n   \n",
      "47  Russ\\nA stupid question, but I use it quite of...   \n",
      "48  Russ\\nA stupid question, but I use it quite of...   \n",
      "49  Russ\\nA stupid question, but I use it quite of...   \n",
      "50  I had the same problem, I want to install my p...   \n",
      "51  I had the same problem, I want to install my p...   \n",
      "52  I had the same problem, I want to install my p...   \n",
      "53  I had the same problem, I want to install my p...   \n",
      "54  I had the same problem, I want to install my p...   \n",
      "55  Hi Adam,\\nThe symptom is not quite the same. T...   \n",
      "56  Hi Adam,\\nThe symptom is not quite the same. T...   \n",
      "57  Hi Adam,\\nThe symptom is not quite the same. T...   \n",
      "58  Hi Adam,\\nThe symptom is not quite the same. T...   \n",
      "59  Hi Adam,\\nThe symptom is not quite the same. T...   \n",
      "60  How would you make clone.CloneMap() generic fo...   \n",
      "61  How would you make clone.CloneMap() generic fo...   \n",
      "62  How would you make clone.CloneMap() generic fo...   \n",
      "63  How would you make clone.CloneMap() generic fo...   \n",
      "64  How would you make clone.CloneMap() generic fo...   \n",
      "65  How would you make clone.CloneMap() generic fo...   \n",
      "66  How would you make clone.CloneMap() generic fo...   \n",
      "67  How would you make clone.CloneMap() generic fo...   \n",
      "\n",
      "                         replier thread_no  \n",
      "0                      Russ Cox          0  \n",
      "1                      OwlHuntr          1  \n",
      "2                  Isaac Wagner          1  \n",
      "3                      abiosoft          2  \n",
      "4             Frederik Deweerdt          2  \n",
      "5                           jqb          3  \n",
      "6              Ian Lance Taylor          4  \n",
      "7                    ziyu_huang          4  \n",
      "8   \"Dimiter \\\"malkia\\\" Stanev\"          4  \n",
      "9   \"Dimiter \\\"malkia\\\" Stanev\"          4  \n",
      "10  \"Dimiter \\\"malkia\\\" Stanev\"          4  \n",
      "11            Geoffrey Clements          6  \n",
      "12            \"Devon H. O'Dell\"          6  \n",
      "13            Geoffrey Clements          6  \n",
      "14                 baldmountain          6  \n",
      "15                     Russ Cox          6  \n",
      "16            \"Devon H. O'Dell\"          6  \n",
      "17                 Rowan Davies          6  \n",
      "18               Bob Cunningham          7  \n",
      "19               Joseph Stewart          7  \n",
      "20               Bob Cunningham          7  \n",
      "21                  Pete Wilson          7  \n",
      "22                 Daniel Dilts          8  \n",
      "23  \"Dimiter \\\"malkia\\\" Stanev\"          8  \n",
      "24                    i3dmaster          8  \n",
      "25                   ziyu_huang          8  \n",
      "26                    i3dmaster          8  \n",
      "27            gorgo...@online.de         8  \n",
      "28                        SnakE          8  \n",
      "29                 Rowan Davies          9  \n",
      "..                           ...       ...  \n",
      "38                    Evan Shaw         10  \n",
      "39             Ian Lance Taylor         12  \n",
      "40                          ray         12  \n",
      "41                       Mad Go         13  \n",
      "42                  Ben Bullock         13  \n",
      "43                  Ben Bullock         13  \n",
      "44                       Mad Go         13  \n",
      "45                       Mad Go         13  \n",
      "46                   ziyu_huang         14  \n",
      "47                 baldmountain         14  \n",
      "48             Ian Lance Taylor         14  \n",
      "49                Esko Luontola         14  \n",
      "50                 Daniel Dilts         15  \n",
      "51             Ian Lance Taylor         15  \n",
      "52                 Daniel Dilts         15  \n",
      "53                 Daniel Dilts         15  \n",
      "54                     Russ Cox         15  \n",
      "55                     Russ Cox         16  \n",
      "56                 Adam Langley         16  \n",
      "57                   ziyu_huang         16  \n",
      "58                   ziyu_huang         16  \n",
      "59                     Russ Cox         16  \n",
      "60                       Rick R         17  \n",
      "61                        SnakE         17  \n",
      "62                       Rick R         17  \n",
      "63                       Rick R         17  \n",
      "64                       Jessta         17  \n",
      "65              Rick Richardson         17  \n",
      "66                       atomly         17  \n",
      "67                     OwlHuntr         17  \n",
      "\n",
      "[68 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thread list no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "[0, 1, 1, 2, 2, 3, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12, 12, 13, 13, 13, 13, 13, 14, 14, 14, 14, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17]\n"
     ]
    }
   ],
   "source": [
    "h = df.replier\n",
    "pprint(len(h))\n",
    "h=list(h)\n",
    "\n",
    "thread_no_list = df.thread_no\n",
    "thread_no_list = list(thread_no_list)\n",
    "print(thread_no_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15 25 14 20 14 20 27 10 31 34 25 13 37  1  1  1 35 10 11  0 11 29 25  0\n",
      " 24 21  5 17  5 21 33  6  1 33 37 33 31 26  2 24 12 37 33  6  7  6 12 26\n",
      "  6  9 32 30 13 36 18 18  4  4 18 18 25 37 29 13  8 19  6 13  6  6 25 37\n",
      " 25  3 37 37 25 22 22 26 22 22 16 23 28 20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niki/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold = sys.maxsize)\n",
    "val = np.array(users)\n",
    "w = open('one_hot.txt','w')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(val)\n",
    "print(integer_encoded)\n",
    "user_indices = integer_encoded\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded),1)\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(integer_encoded)\n",
    "output_shape = one_hot_encoded.shape[1]\n",
    "w.write(str(one_hot_encoded))\n",
    "w.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max no of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15 25 14 20 14 20 27 10 31 34 25 13 37  1  1  1 35 10 11  0 11 29 25  0\n",
      " 24 21  5 17  5 21 33  6  1 33 37 33 31 26  2 24 12 37 33  6  7  6 12 26\n",
      "  6  9 32 30 13 36 18 18  4  4 18 18 25 37 29 13  8 19  6 13  6  6 25 37\n",
      " 25  3 37 37 25 22 22 26 22 22 16 23 28 20]\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "print(user_indices)\n",
    "input_weights = []\n",
    "user_size = max(user_indices) + 1\n",
    "print(user_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding of user - User Vector (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "onehot_encoded_final = []\n",
    "for replier in h:\n",
    "    idx1 = label_encoder.transform([replier])\n",
    "    onehot = np.zeros(user_size)\n",
    "    onehot[idx1] = 1\n",
    "    onehot_encoded_final.append(list(onehot))\n",
    "one_hot_encoded_final = np.array(onehot_encoded_final)\n",
    "print(type(one_hot_encoded_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary encoding of users participating in each thread (concat vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1.],\n",
      "       [0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1.],\n",
      "       [0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1.],\n",
      "       [0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 1.],\n",
      "       [0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 2., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 2., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 3., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 3., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 3., 0., 0., 0., 1.],\n",
      "       [0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 3., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 2., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 2., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "index=0\n",
    "weight_list = []\n",
    "for i in range(0, max(thread_no_list)+1):\n",
    "    temp_index=index\n",
    "    array  = np.zeros(user_size)\n",
    "    for j in range(temp_index, temp_index + thread_no_list.count(i)):\n",
    "        array[user_indices[j]] += 1\n",
    "        weight_list.append(list(array))\n",
    "        index+=1\n",
    "\n",
    "weights = np.array(weight_list)\n",
    "pprint(weights) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df.body\n",
    "# print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find max length of body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest(l):\n",
    "    m=0\n",
    "    for k in l:\n",
    "        m = max(len(k),m)\n",
    "    return m\n",
    "max_len = longest(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization & conversion to sequence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "# max_words = 1294\n",
    "# max_len = 3267\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x_train)\n",
    "sequences = tok.texts_to_sequences(x_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "print(type(sequences_matrix))\n",
    "print(len(sequences_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.zeros(user_size)\n",
    "a=tf.convert_to_tensor(b)\n",
    "#dense_cat = Dense(256, activation='relu')(a)\n",
    "#flat1 = Dense(32, activation='relu')(dense_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.merge import concatenate\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    inputs2 = Input(name='inputs2',shape=[user_size])\n",
    "    layer2=Dense(256,name='FC2')(inputs2)\n",
    "    \n",
    "    merge=concatenate([layer,layer2])\n",
    "    \n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(output_shape,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=[inputs,inputs2],outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 3182)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 3182, 50)          64700     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 38)                9766      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 38)                0         \n",
      "=================================================================\n",
      "Total params: 120,546\n",
      "Trainable params: 120,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54 samples, validate on 14 samples\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 7s 124ms/step - loss: 0.6931 - acc: 0.5015 - val_loss: 0.6880 - val_acc: 0.9605\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 6s 106ms/step - loss: 0.6876 - acc: 0.8796 - val_loss: 0.6802 - val_acc: 0.9737\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 6s 103ms/step - loss: 0.6795 - acc: 0.9196 - val_loss: 0.6648 - val_acc: 0.9737\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 5s 84ms/step - loss: 0.6613 - acc: 0.9440 - val_loss: 0.6212 - val_acc: 0.9737\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 5s 98ms/step - loss: 0.6104 - acc: 0.9527 - val_loss: 0.3650 - val_acc: 0.9737\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 6s 105ms/step - loss: 0.3676 - acc: 0.9605 - val_loss: 0.1845 - val_acc: 0.9737\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 6s 107ms/step - loss: 0.2003 - acc: 0.9688 - val_loss: 0.1481 - val_acc: 0.9737\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 5s 90ms/step - loss: 0.1625 - acc: 0.9712 - val_loss: 0.1353 - val_acc: 0.9737\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 5s 92ms/step - loss: 0.1455 - acc: 0.9722 - val_loss: 0.1317 - val_acc: 0.9737\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 5s 102ms/step - loss: 0.1410 - acc: 0.9732 - val_loss: 0.1309 - val_acc: 0.9737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f40805520b8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([sequences_matrix,weights],one_hot_encoded_final,batch_size=128,epochs=10,\n",
    "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
